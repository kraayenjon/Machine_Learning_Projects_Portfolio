{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0KgqKO9Y3YE"
   },
   "source": [
    "# In part 2, we plan to use publically available deep embeddings of text in order to predict emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d1740tOHm3bz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Xa0Q6Fm2q6nu"
   },
   "outputs": [],
   "source": [
    "# Here we need both HuggingFace's \"datasets\" and \"sentence-transformers\" libraries.\n",
    "# Explanations to follow in the code.\n",
    "\n",
    "#!pip install datasets\n",
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "otnZptz0rHaY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Let's download a dataset of English tweets, with an \"emotion\" label attached\n",
    "# to each tweet, as we did in part 1.\n",
    "from datasets import load_dataset, Dataset\n",
    "emotions = load_dataset(\"SetFit/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FT0HjVLLrKwl"
   },
   "outputs": [],
   "source": [
    "train = emotions[\"train\"].to_pandas()\n",
    "val = emotions[\"validation\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPWzV3IYCyKt"
   },
   "source": [
    "# Converting text to \"deep embeddings\".\n",
    "Using HuggingFace's SentenceTransformer platform, we will convert the tweets to 384 dimensional vectors.  This mysterious embedding was trained from hundreds of millions of sentences available on the web, precisely for the purpose of being able to semantically compare sentences (to tell whether two sentences mean roughly the same).  \n",
    "\n",
    "We do not learn in ML II how to train such sentence-transformers, but we are allowed to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nwjbes8OrL2u"
   },
   "outputs": [],
   "source": [
    "def convert_texts_to_deep_embeddings(texts):\n",
    "  from sentence_transformers import SentenceTransformer\n",
    "\n",
    "  model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "  # documentation: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "  n = len(texts)\n",
    "  ret = []\n",
    "\n",
    "# Work in batches of 100 tweets, to avoid memory problems\n",
    "  for i in range(0, n, 100):\n",
    "    embeddings = model.encode(list(texts[i:min(i+100, n)]))\n",
    "    ret.append(embeddings)\n",
    "  return np.concatenate(ret, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waK8fJH9YAnN"
   },
   "outputs": [],
   "source": [
    "# The first time you execute the embedding, some data will download to your\n",
    "# filesystem.  This data, which contains hundreds of millions of bytes,\n",
    "# will be cached by HuggingFace library for the next uses.\n",
    "\n",
    "val_embedded = convert_texts_to_deep_embeddings(val[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCQ6YYwDYDrk"
   },
   "outputs": [],
   "source": [
    "train_embedded = convert_texts_to_deep_embeddings(train[\"text\"])\n",
    "print (f\"Embedding dimension = {train_embedded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxF3aEDLmIra"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 5\n",
    "clf = KNeighborsClassifier(n_neighbors = k, metric='cosine')\n",
    "clf.fit(train_embedded, train[\"label_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plMhBCDcmdJr"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss # count number of times class is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxIIVhPnmXl7"
   },
   "outputs": [],
   "source": [
    "train_preds = clf.predict(train_embedded)\n",
    "hamming_loss(train[\"label_text\"], train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ez8Iab8mmaFS"
   },
   "outputs": [],
   "source": [
    "val_preds = clf.predict(val_embedded)\n",
    "hamming_loss(val[\"label_text\"], val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zCQIHoIEUkB"
   },
   "source": [
    "**Discussion point:**  Compare results to the best you could get with 384 features in part 1 of the notebook.\n",
    "\n",
    "**Discussion point:** Plot a confusion matrix, as done in part 1 of the notebook (feel free to reuse code from the forum threads.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZYX03M-EuGZ"
   },
   "source": [
    "# Let's use PCA to further reduce the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOi9xIprmsaf"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's start with a full PCA, to get the \"scree\" plot.\n",
    "fitted_pca = PCA().fit(train_embedded)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fitted_pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UblBrZMwETPp"
   },
   "source": [
    "**Discussion point:** As done in part 1, reduce the dimension to various values below 384, and see how it affects the accuracy.  Compare with part 1 of the notebook (bad of words), to see whether you can get the same accuracy, dimensionality being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L8E0cwnFbKu"
   },
   "source": [
    "One of the fun things you can do with PCA on few dimensions, is  visualization.  Let's try to visualize the data in 2d, using the first 5 components of PCA, and see if we get any insights.  I believe that it is possible to see some separation between sadness and joy, which are two extremes, in this mapping.\n",
    "\n",
    "\n",
    "**Discussion Point:** Can you find other visual-semantic phenomena?  Feel free to do 3d plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmLJbPTunGCj"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5).fit(train_embedded)\n",
    "train_reduced = pca.transform(train_embedded)\n",
    "plt.figure(figsize=(10,10))\n",
    "import matplotlib.pyplot as plt\n",
    "for emotion, color in [('sadness', 'black'), ('joy', 'orange')]:\n",
    "  indices = (train['label_text'] == emotion)\n",
    "  plt.plot(train_reduced[indices,1], train_reduced[indices, 2], '.', color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHAgT2e5JUzW"
   },
   "source": [
    "# Taking the deep embeddings up a notch\n",
    "The dimensionality of 384 is quite small, given the monster models that are out there today.  \n",
    "For a list of examples, see this page:\n",
    "https://huggingface.co/sentence-transformers\n",
    "\n",
    "In order to use other deep sematic embedding models, replace the string \"sentence-transformers/all-MiniLM-L6-v2\" with the string corresponding to the model you want to try. Note that some models will take a long time to run the embedding, and will also require much disk space for download.  If you own a GPU, or using a GPU environment on the cloud, you can take advantage and make the embedding run much faster.  Feel free to find examples online for running HuggingFace's sentence-transformers on GPU, or ask me.\n",
    "\n",
    "Also note that \"bigger\" is not always \"better\" - some of the sentence-transformers may be bigger than the one we use here, but may not perform as well.  \n",
    "\n",
    "**Advanced Discussion Point:** Feel free to try other sentence-transformer semantic embedding models, and share your findings with everyone!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
